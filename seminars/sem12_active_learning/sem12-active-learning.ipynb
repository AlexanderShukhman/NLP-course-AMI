{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Активное обучение $-$ класс алгоритмов обучения моделей машинного обучения. Алгоритмы активного обучения отличаются тем, могут интерактивно запрашивать пользователя (или некоторый другой источник информации) для разметки новых примеров данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"active_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pool-Based Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом сценарии экземпляры извлекаются из всего пула данных и им присваивается информативная оценка, которая показывает, насколько хорошо текущий алгоритм «понимает» данные. \n",
    "\n",
    "Затем система выбираются и размечаются наиболее информативные примеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncertainty sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В рамках этого алгоритма размечаются те примеры, на которых текущая модель наименее уверена.\n",
    "\n",
    "В качестве функций \"уверенности\" можно использовать вероятности классов или расстояния до разделяющей гиперплоскости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membership Query Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь алгритм обучения модели генерирует свои собственные примеры из некоторого настраиваемого распределения. \n",
    "Эти сгенерированные примеры отправляются на разметку и модель дообучается с учетом разметки этих примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query by Committee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: построить ансамбль моделей $a_1,...,a_T$. \n",
    "\n",
    "Выбирать новые объекты $x_i$ с наибольшей несогласованностью решений ансамбля моделей.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принцип максимума энтропии: выбираем $x_i$, на котором $a_t(x_i)$ максимально различны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принцип максимума средней $KL$-дивергенции:выбираем $x_i$ , на котором $P_t(y|x_i)$ максимально различны:\n",
    "\n",
    "$С(y|u) = \\frac{1}{T}\\sum_{t=1}^T P_t(y|u)$ - консенсус комитета "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM для Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые активные алгоритмы обучения построены на алгоритме SVM и используют структуру SVM для определения того, какие точки данных нужно размечать. \n",
    "\n",
    "SVM используется для определения уверенности модели в предсказании на каждом из примеров выборки. \n",
    "В качестве меры уверенности служит расстояние от объекта до построенной не текущей итерации разделяющей гиперплоскости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning for texts classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим алгоритм pool-based active learning на примере задачи классификации твитов по тональности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Разделить данные на X_pool (выборка, которую можно размечать) и X_test.\n",
    "2. Выбрать $k$ примеров из X_pool для начального X_train и разметить их. Остальные данные в X_pool $-$ валидационное множество. \n",
    "3. Обучить модель на  X_train.\n",
    "5. Сделать predict обученной моделью на X_pool, вычислить вероятности для каждого $x_i$.\n",
    "6. Вычислить качество работы модели на X_test.\n",
    "7. Выбрать $k$ наиболее информативных объектов из X_pool, основываясь на уверенности модели в каждом из объектов (например, вероятности классов).\n",
    "8. Переменести эти $k$ выбранных объектов в X_train.\n",
    "9. Если качество работы модели на X_test достаточное, то останавливаемся, иначе возвращаемся к шагу 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import ngrams\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam_text_classification_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = [0 if category == 'ham' else 1 for category in df['Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_test,y,y_test = train_test_split(np.array(df['Message']), np.array(df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence(class_probs):\n",
    "    return abs(0.5-class_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94      1338\n",
      "           1       0.25      0.91      0.39        55\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1393\n",
      "   macro avg       0.62      0.90      0.66      1393\n",
      "weighted avg       0.97      0.89      0.92      1393\n",
      "\n",
      "60 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95      1292\n",
      "           1       0.42      0.84      0.56       101\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1393\n",
      "   macro avg       0.70      0.88      0.75      1393\n",
      "weighted avg       0.95      0.91      0.92      1393\n",
      "\n",
      "70 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1250\n",
      "           1       0.61      0.85      0.71       143\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1393\n",
      "   macro avg       0.79      0.89      0.83      1393\n",
      "weighted avg       0.94      0.93      0.93      1393\n",
      "\n",
      "80 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      1237\n",
      "           1       0.71      0.91      0.80       156\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1393\n",
      "   macro avg       0.85      0.93      0.88      1393\n",
      "weighted avg       0.96      0.95      0.95      1393\n",
      "\n",
      "90 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1214\n",
      "           1       0.80      0.90      0.85       179\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1393\n",
      "   macro avg       0.89      0.93      0.91      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "100 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      1222\n",
      "           1       0.79      0.92      0.85       171\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1393\n",
      "   macro avg       0.89      0.94      0.91      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "110 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1229\n",
      "           1       0.79      0.96      0.87       164\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1393\n",
      "   macro avg       0.89      0.96      0.92      1393\n",
      "weighted avg       0.97      0.96      0.97      1393\n",
      "\n",
      "120 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      1233\n",
      "           1       0.77      0.97      0.86       160\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1393\n",
      "   macro avg       0.88      0.97      0.92      1393\n",
      "weighted avg       0.97      0.96      0.97      1393\n",
      "\n",
      "130 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1213\n",
      "           1       0.82      0.92      0.87       180\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1393\n",
      "   macro avg       0.90      0.94      0.92      1393\n",
      "weighted avg       0.97      0.96      0.96      1393\n",
      "\n",
      "140 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      1221\n",
      "           1       0.84      0.98      0.90       172\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1393\n",
      "   macro avg       0.92      0.97      0.94      1393\n",
      "weighted avg       0.98      0.97      0.97      1393\n",
      "\n",
      "150 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      1226\n",
      "           1       0.82      0.99      0.90       167\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1393\n",
      "   macro avg       0.91      0.98      0.94      1393\n",
      "weighted avg       0.98      0.97      0.97      1393\n",
      "\n",
      "160 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99      1226\n",
      "           1       0.83      0.99      0.90       167\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1393\n",
      "   macro avg       0.91      0.98      0.94      1393\n",
      "weighted avg       0.98      0.97      0.98      1393\n",
      "\n",
      "170 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99      1223\n",
      "           1       0.85      1.00      0.92       170\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.92      0.99      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "180 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1221\n",
      "           1       0.86      1.00      0.92       172\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.93      0.99      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "190 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1216\n",
      "           1       0.87      0.99      0.93       177\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.93      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "200 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1216\n",
      "           1       0.87      0.99      0.93       177\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.93      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "210 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1215\n",
      "           1       0.87      0.98      0.92       178\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.93      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "220 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1217\n",
      "           1       0.88      1.00      0.93       176\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.99      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "230 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1215\n",
      "           1       0.89      1.00      0.94       178\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.99      0.96      1393\n",
      "weighted avg       0.99      0.98      0.98      1393\n",
      "\n",
      "240 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1214\n",
      "           1       0.88      0.99      0.93       179\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "250 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1209\n",
      "           1       0.89      0.97      0.93       184\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "260 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1214\n",
      "           1       0.88      0.99      0.93       179\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "270 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1214\n",
      "           1       0.88      0.99      0.93       179\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "280 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1212\n",
      "           1       0.89      0.98      0.93       181\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "290 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1215\n",
      "           1       0.88      0.99      0.93       178\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1215\n",
      "           1       0.88      0.99      0.93       178\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.99      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "310 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1214\n",
      "           1       0.89      0.99      0.94       179\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.99      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "320 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1213\n",
      "           1       0.89      0.99      0.93       180\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.98      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "330 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1213\n",
      "           1       0.89      0.99      0.94       180\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.99      0.97      1393\n",
      "weighted avg       0.99      0.98      0.98      1393\n",
      "\n",
      "340 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1212\n",
      "           1       0.89      0.99      0.94       181\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.99      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "350 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1211\n",
      "           1       0.90      0.99      0.95       182\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.95      0.99      0.97      1393\n",
      "weighted avg       0.99      0.98      0.99      1393\n",
      "\n",
      "360 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1212\n",
      "           1       0.90      0.99      0.94       181\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.95      0.99      0.97      1393\n",
      "weighted avg       0.99      0.98      0.98      1393\n",
      "\n",
      "370 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1209\n",
      "           1       0.91      0.99      0.95       184\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1393\n",
      "   macro avg       0.95      0.99      0.97      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = 50\n",
    "\n",
    "dataset_size = X.shape[0]\n",
    "target_score = 0.95\n",
    "score = 0\n",
    "step = 10\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_pool = X[train_size:]\n",
    "y_pool = y[train_size:]\n",
    "\n",
    "scores = [0]\n",
    "train_szs = [0]\n",
    "\n",
    "while score < target_score and train_size <= dataset_size:\n",
    "    vec = CountVectorizer(ngram_range=(1, 1))\n",
    "    bow = vec.fit_transform(X_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf = clf.fit(bow,y_train)\n",
    "    pred = clf.predict(vec.transform(X_test))\n",
    "    \n",
    "    print(\"{0} train samples\".format(train_size))\n",
    "    print(classification_report(pred, y_test))\n",
    "    score = f1_score(pred, y_test)\n",
    "    scores.append(score)\n",
    "    train_szs.append(train_size)\n",
    "    \n",
    "    pred_probs = clf.predict_proba(vec.transform(X_pool))\n",
    "    confidences = [get_confidence(probs) for probs in pred_probs]\n",
    "    \n",
    "    X_train = np.concatenate([X_train, X_pool[np.argsort(confidences)[:step]]])\n",
    "    y_train = np.concatenate([y_train, y_pool[np.argsort(confidences)[:step]]])\n",
    "    X_pool = X_pool[sorted(np.argsort(confidences)[step:])]\n",
    "    y_pool = y_pool[sorted(np.argsort(confidences)[step:])]\n",
    "    train_size += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179 train samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1215\n",
      "           1       0.88      0.99      0.93       178\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1393\n",
      "   macro avg       0.94      0.99      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(X)\n",
    "clf = clf.fit(bow,y)\n",
    "pred = clf.predict(vec.transform(X_test))\n",
    "\n",
    "print(\"{0} train samples\".format(dataset_size))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2721c0f0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_szs,scores)\n",
    "plt.plot(train_szs, [0.95 for sz in train_szs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что для достижения лучшего качества на этом датасете дсотаточно обучиться на 300 правильно выбранных примерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
